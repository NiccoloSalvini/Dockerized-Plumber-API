---
title: "Dockerized API Scraping [Immobiliare.it](https://www.immobiliare.it/)"
output: 
  github_document:
    toc: TRUE
    toc_depth: 3
    fig_width: 5
    fig_height: 5
---

<!-- README.md is generated from README.Rmd. Please edit that file -->




```{r global.options, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,  
  strip.white = TRUE,                 # if FALSE knitr will not remove white spaces at the beg or end
  fig.path = "img/",                  # file path to the directory DESTINATION where knitr shall store the
  fig.width=12,                       # the width for plots created by code chunk
  fig.height=8,                       # the height for plots created by code chunk
  cache = FALSE                       # if TRUE knitr will cache the results to reuse in future knits
)


knitr::knit_hooks$set(imgcenter = function(before, options, envir){  # to center image 
  if (before) {                                                      # add imgcenter = TRUE
    htmltools::HTML("<p align='center'>")                            # to the chunk options
  } else {
    htmltools::HTML("</p>")
  }
})
```



<img src="img/logo.png" align="right" height="80" />
 
# API Infrastructure

_author_: **[Niccol√≤ Salvini](https://niccolosalvini.netlify.app/)**
_date_: `r format(Sys.Date(), "%d %B, %Y")`


<br> <br> 






These REST APIs provide a way for platform/language independent access to the public [Immobiliare.it](https://www.immobiliare.it/) dataset of real estate rental market. The dataset is going to be updated  daily, some of the entries can be deleted and replaced by some other. In order to keep track to changes in the dataset I strongly suggest to store the daily meausuraments and then merge them. In the final stage of the API building Th user can access this data and a scheduler job fetches new data in 12 hour intervals from Immobiliare through the R script `plumber.R` and adds new rows to the saved dataset (not implemented yet). The data comes in a json format and by the parameter npages can be selected the number of pages the user needs to scrap. The data import via Cron and the APIs are run seperately in two Docker containers with a shared volume for the data as specified in the `docker-compose.yml`.<br><br>


```{r FIRENZE, echo=FALSE, imgcenter = TRUE, fig.retina=1}
knitr::include_graphics("img/apiinfr.png") 
```


## API Documentation:  

- Get complete data from all Indian measurement points   

      GET */scrapecat

      param npages number of pages that are going to be scraped
      content-type: application/json 

- Get all Indian measurement locations 

      GET */scrapelink

      param nlink number of link that are going to be scraped
      content-type: application/json 

 